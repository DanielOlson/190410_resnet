{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet\n",
    "\n",
    "## Please watch Ng C4W2L01-C4W2L04, the first of which is found [here](https://www.youtube.com/watch?v=-bvTzZCEOdM&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=12).\n",
    "\n",
    "The convolutional neural network that we developed and ran was adequate for use on a small problem with a few classes, but it lacks the explanatory power to produce highly accurate results for more difficult datasets.  Instead, more interesting neural networks have been developed which have greater explanatory power.  One of the most powerful architectures today is called ResNet, which is short for residual network.  \n",
    "\n",
    "In principle, you could take the network that you've been working on and make it more flexible by adding more convolutional layers, which is to say that we could add more sequences of feature map generation.  This is what is meant when people use the term \"deep\" learning.  However, if you did this, you would quickly run into the problem that your network would struggle to learn weights in the lower (closer to the inputs) layers of the network.  This is a result of the way that neural networks are trained.  In particular they rely on the ability to take the derivative of a misfit function (e.g. least squares) with respect to a parameter, and to adjust the weight based on that derivative.  However in (naive) deep networks, this gradient has the tendency to become negligibly small as the impact of that weight gets lost in the myriad layers of convolutions and activations closer to the output.  \n",
    "\n",
    "ResNet solves this problem by ensuring that the information in each weight gets propagated to the output.  It does this by simply adding the layer's input to each layer's output, so instead of \n",
    "$$\n",
    "\\mathbf{x}_{l+1} = \\mathcal{F}_{l}(\\mathbf{x}_l),\n",
    "$$\n",
    "at each layer, the neural network performs the operation\n",
    "$$\n",
    "\\mathbf{x}_{l+1} = \\mathcal{F}_{l}(\\mathbf{x}_l) + \\mathbf{x}_l.\n",
    "$$\n",
    "Rearranging this equation, we can see why this architecture is called a residual network:\n",
    "$$\n",
    "\\mathbf{x}_{l+1} - \\mathbf{x}_l = \\mathcal{F}_{l}(\\mathbf{x}_l).\n",
    "$$\n",
    "Each layer is modeling the residual between consecutive feature maps.  The pedantic amongst us will note that this only works when the output of $\\mathcal{F}_{l}(\\mathbf{x}_l)$ is the same size as the input.  This is dealt with by performing a suitable linear transformation on $\\mathbf{x}_l$, making the equation\n",
    "$$\n",
    "\\mathbf{x}_{l+1} = \\mathcal{F}_{l}(\\mathbf{x}_l) + W \\mathbf{x}_l,\n",
    "$$\n",
    "where $W$ is a matrix that has learnable weights.  The matrix $W$ is most often formulated as a convolution with a 1x1 kernel size.   \n",
    "\n",
    "The addition of the input is known as a *skip connection* because it looks like this:\n",
    "<img src=res_net.svg width=600/>\n",
    "The input is run through a normal conv layer (perhaps several) and then added to the output, where it can then be maxpooled or run through an activation or whatever.  \n",
    "\n",
    "Keras makes these sorts of networks pretty easy to program.  To start with, let's apply this network to the CIFAR-10 classification problem, but we'll do it for all 10 classes.  All the non-model definition code should look the same as our previous example.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.datasets as kd\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = kd.cifar10.load_data()\n",
    "labels = ['airplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "N = len(labels)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, N)\n",
    "y_test = keras.utils.to_categorical(y_test, N)\n",
    "\n",
    "import keras.models as km\n",
    "model = km.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now things get more interesting.  Obviously, ResNet as described above is more of a concept than a specific architecture: we'll need to make some more specific design choices.  One good way of doing this is to look at the literature and copy what others have done.  In particular, the [original ResNet Paper](https://arxiv.org/abs/1512.03385) provides an example of ResNet being applied to CIFAR-10 that yielded excellent accuracy.  Let's emulate their network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
